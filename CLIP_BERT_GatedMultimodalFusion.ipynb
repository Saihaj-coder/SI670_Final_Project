{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cC-nmXJYEVFT"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import cv2\n",
        "import torch\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import albumentations as A\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from transformers import CLIPProcessor, CLIPModel, BertModel, BertTokenizer, get_linear_schedule_with_warmup\n",
        "from sklearn.metrics import accuracy_score, f1_score, classification_report\n",
        "from tqdm import tqdm\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_df = pd.read_csv('/content/drive/MyDrive/memo_3/memotion3/memotion3/train.csv')"
      ],
      "metadata": {
        "id": "o6iwo-nnEbDI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_df['overall'] = train_df['overall'].replace({\n",
        "                                                            'very_positive': 2,\n",
        "                                                            'positive': 2,\n",
        "                                                            'neutral': 1,\n",
        "                                                            'very_negative': 0,\n",
        "                                                            'negative': 0})"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BsF9D3YAE2xn",
        "outputId": "950beeec-28a7-422c-b75e-7917a1cda1d9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-3740866416.py:1: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
            "  train_df['overall'] = train_df['overall'].replace({\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "val_df = pd.read_csv('/content/drive/MyDrive/memo_3/val.csv')"
      ],
      "metadata": {
        "id": "BeHEZ8dQE433"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "val_df['overall'] = val_df['overall'].replace({\n",
        "                                                            'very_positive': 2,\n",
        "                                                            'positive': 2,\n",
        "                                                            'neutral': 1,\n",
        "                                                            'very_negative': 0,\n",
        "                                                            'negative': 0})"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EACGyDY6E7nB",
        "outputId": "a976db7c-6eb7-495c-87b8-c9a2a2e31d89"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-2159871312.py:1: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
            "  val_df['overall'] = val_df['overall'].replace({\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# !pip install text_hammer"
      ],
      "metadata": {
        "id": "17r3R71hE9aP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import text_hammer as th"
      ],
      "metadata": {
        "id": "bEJgJEYMFAVP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "\n",
        "from tqdm._tqdm_notebook import tqdm_notebook\n",
        "tqdm_notebook.pandas()\n",
        "\n",
        "def text_preprocessing(df, col_name):\n",
        "  column = col_name\n",
        "  df[column] = df[column].progress_apply(lambda x:str(x).lower())\n",
        "  df[column] = df[column].progress_apply(lambda x: th.cont_exp(x)) # you're -> you are; we'll be -> we will be\n",
        "  df[column] = df[column].progress_apply(lambda x: th.remove_emails(x))\n",
        "  df[column] = df[column].progress_apply(lambda x: th.remove_html_tags(x))\n",
        "\n",
        "  df[column] = df[column].progress_apply(lambda x: th.remove_special_chars(x))\n",
        "  df[column] = df[column].progress_apply(lambda x: th.remove_accented_chars(x))\n",
        "\n",
        "  return df"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CoLX62lAFFkM",
        "outputId": "a563957a-3349-4e16-d9bf-84cefb6082c8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CPU times: user 570 µs, sys: 112 µs, total: 682 µs\n",
            "Wall time: 691 µs\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset = text_preprocessing(train_df, 'ocr')\n",
        "val_dataset = text_preprocessing(val_df, 'ocr')"
      ],
      "metadata": {
        "id": "JWkKtRCAFJb2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 362
        },
        "id": "sSeGXGxlFLKd",
        "outputId": "c8b8fd58-a82d-4412-9722-17882f2dfbcf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "   Unnamed: 0                                          image_url      humour  \\\n",
              "0           0  https://encrypted-tbn0.gstatic.com/images?q=tb...  very_funny   \n",
              "1           1  https://encrypted-tbn0.gstatic.com/images?q=tb...  very_funny   \n",
              "2           2  https://encrypted-tbn0.gstatic.com/images?q=tb...       funny   \n",
              "3           3  https://preview.redd.it/iwcz3o2niix61.jpg?widt...  very_funny   \n",
              "4           4  https://encrypted-tbn0.gstatic.com/images?q=tb...   not_funny   \n",
              "\n",
              "         sarcastic       offensive      motivational  overall  \\\n",
              "0  twisted_meaning   not_offensive  not_motivational        1   \n",
              "1  twisted_meaning          slight  not_motivational        1   \n",
              "2          general   not_offensive  not_motivational        2   \n",
              "3  twisted_meaning   not_offensive  not_motivational        2   \n",
              "4    not_sarcastic  very_offensive  not_motivational        0   \n",
              "\n",
              "                                                 ocr  \n",
              "0  relationship statussinglemarriedit is complica...  \n",
              "1  bernie or reaganbe informed compare them on th...  \n",
              "2  i hear somethingbetter run a mileoh my god raj...  \n",
              "3  redditors visiting ig memepagesdies from cring...  \n",
              "4  samajh nahi aaya par sun ke acabobudget speech...  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-92925763-1ab8-4e50-af60-276fc981e466\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Unnamed: 0</th>\n",
              "      <th>image_url</th>\n",
              "      <th>humour</th>\n",
              "      <th>sarcastic</th>\n",
              "      <th>offensive</th>\n",
              "      <th>motivational</th>\n",
              "      <th>overall</th>\n",
              "      <th>ocr</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>https://encrypted-tbn0.gstatic.com/images?q=tb...</td>\n",
              "      <td>very_funny</td>\n",
              "      <td>twisted_meaning</td>\n",
              "      <td>not_offensive</td>\n",
              "      <td>not_motivational</td>\n",
              "      <td>1</td>\n",
              "      <td>relationship statussinglemarriedit is complica...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>https://encrypted-tbn0.gstatic.com/images?q=tb...</td>\n",
              "      <td>very_funny</td>\n",
              "      <td>twisted_meaning</td>\n",
              "      <td>slight</td>\n",
              "      <td>not_motivational</td>\n",
              "      <td>1</td>\n",
              "      <td>bernie or reaganbe informed compare them on th...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2</td>\n",
              "      <td>https://encrypted-tbn0.gstatic.com/images?q=tb...</td>\n",
              "      <td>funny</td>\n",
              "      <td>general</td>\n",
              "      <td>not_offensive</td>\n",
              "      <td>not_motivational</td>\n",
              "      <td>2</td>\n",
              "      <td>i hear somethingbetter run a mileoh my god raj...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3</td>\n",
              "      <td>https://preview.redd.it/iwcz3o2niix61.jpg?widt...</td>\n",
              "      <td>very_funny</td>\n",
              "      <td>twisted_meaning</td>\n",
              "      <td>not_offensive</td>\n",
              "      <td>not_motivational</td>\n",
              "      <td>2</td>\n",
              "      <td>redditors visiting ig memepagesdies from cring...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>4</td>\n",
              "      <td>https://encrypted-tbn0.gstatic.com/images?q=tb...</td>\n",
              "      <td>not_funny</td>\n",
              "      <td>not_sarcastic</td>\n",
              "      <td>very_offensive</td>\n",
              "      <td>not_motivational</td>\n",
              "      <td>0</td>\n",
              "      <td>samajh nahi aaya par sun ke acabobudget speech...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-92925763-1ab8-4e50-af60-276fc981e466')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-92925763-1ab8-4e50-af60-276fc981e466 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-92925763-1ab8-4e50-af60-276fc981e466');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    <div id=\"df-559e7c2b-17c9-408d-8499-38e39a0b081a\">\n",
              "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-559e7c2b-17c9-408d-8499-38e39a0b081a')\"\n",
              "                title=\"Suggest charts\"\n",
              "                style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "      </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "      <script>\n",
              "        async function quickchart(key) {\n",
              "          const quickchartButtonEl =\n",
              "            document.querySelector('#' + key + ' button');\n",
              "          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "          quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "          try {\n",
              "            const charts = await google.colab.kernel.invokeFunction(\n",
              "                'suggestCharts', [key], {});\n",
              "          } catch (error) {\n",
              "            console.error('Error during call to suggestCharts:', error);\n",
              "          }\n",
              "          quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "        }\n",
              "        (() => {\n",
              "          let quickchartButtonEl =\n",
              "            document.querySelector('#df-559e7c2b-17c9-408d-8499-38e39a0b081a button');\n",
              "          quickchartButtonEl.style.display =\n",
              "            google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "        })();\n",
              "      </script>\n",
              "    </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "train_dataset",
              "summary": "{\n  \"name\": \"train_dataset\",\n  \"rows\": 7000,\n  \"fields\": [\n    {\n      \"column\": \"Unnamed: 0\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 2020,\n        \"min\": 0,\n        \"max\": 6999,\n        \"num_unique_values\": 7000,\n        \"samples\": [\n          6500,\n          2944,\n          2024\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"image_url\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 7000,\n        \"samples\": [\n          \"https://preview.redd.it/v1efucmn3uc61.jpg?width=640&crop=smart&auto=webp&s=520d5a7ad8451e0af16d02686fe7a379f7d6a338\",\n          \"https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcShhroFEIh6V-lpdbgvviuOAAMCgKSKCmyerQ&usqp=CAU\",\n          \"https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcSHieHYs3PupXSfRdZeZvV4CQv1RGzNqJTxPg&usqp=CAU\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"humour\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 4,\n        \"samples\": [\n          \"funny\",\n          \"hilarious\",\n          \"very_funny\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"sarcastic\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 4,\n        \"samples\": [\n          \"general\",\n          \"very_twisted\",\n          \"twisted_meaning\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"offensive\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 4,\n        \"samples\": [\n          \"slight\",\n          \"hateful_offensive\",\n          \"not_offensive\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"motivational\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 2,\n        \"samples\": [\n          \"motivational\",\n          \"not_motivational\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"overall\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 0,\n        \"max\": 2,\n        \"num_unique_values\": 3,\n        \"samples\": [\n          1,\n          2\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"ocr\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 6928,\n        \"samples\": [\n          \"the battle over legalimmigration reformandys mennestrump1 illegalsoas\",\n          \"when you grow up and realise bhagwan ne tumhe mummyke pet me nahi dala tha wo process kuch aur thichinmay prabhu rj vivanbeardedkameenakyaa matlab bachpanse chtiya kate jaa raha hai\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from transformers import CLIPModel, CLIPProcessor, BertTokenizer, BertModel, get_linear_schedule_with_warmup\n",
        "import albumentations as A\n",
        "from PIL import Image\n",
        "from tqdm import tqdm\n",
        "from sklearn.metrics import classification_report, confusion_matrix, f1_score\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "from torch.cuda.amp import autocast, GradScaler"
      ],
      "metadata": {
        "id": "yOnanxElFTJ1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ===================== Dataset Class =====================\n",
        "class MemeDataset(Dataset):\n",
        "    def __init__(self, images, captions, sentiments, tokenizer, image_transforms, image_dir):\n",
        "        self.images = images\n",
        "        self.captions = captions\n",
        "        self.sentiments = sentiments\n",
        "        self.tokenizer = tokenizer\n",
        "        self.image_transforms = image_transforms\n",
        "        self.image_dir = image_dir\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.images)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        image_name = self.images[idx]\n",
        "        image_path = os.path.join(self.image_dir, image_name)\n",
        "        caption = self.captions[idx]\n",
        "        sentiment = self.sentiments[idx]\n",
        "\n",
        "        # Load and preprocess image with proper error handling\n",
        "        try:\n",
        "            image = Image.open(image_path).convert('RGB')\n",
        "            image = np.array(image)\n",
        "        except Exception as e:\n",
        "            # print(f\"Error loading {image_path}: {e}\")\n",
        "            image = np.full((224, 224, 3), 128, dtype=np.uint8)\n",
        "\n",
        "        # Apply transforms\n",
        "        image = self.image_transforms(image=image)['image']\n",
        "        image = torch.tensor(image).permute(2, 0, 1).float()\n",
        "\n",
        "        # Ensure caption is a valid string\n",
        "        if not isinstance(caption, str):\n",
        "            caption = str(caption) if caption else \"empty caption\"\n",
        "        if isinstance(caption, list):\n",
        "            caption = ' '.join(caption)\n",
        "        if not caption or caption.strip() == '':\n",
        "            caption = \"empty caption\"\n",
        "\n",
        "        # Encode caption\n",
        "        encoded_caption = self.tokenizer(\n",
        "            caption,\n",
        "            return_tensors=\"pt\",\n",
        "            padding='max_length',\n",
        "            truncation=True,\n",
        "            max_length=77\n",
        "        )\n",
        "        input_ids = encoded_caption['input_ids'].squeeze()\n",
        "        attention_mask = encoded_caption['attention_mask'].squeeze()\n",
        "\n",
        "        sentiment_class = torch.tensor(sentiment, dtype=torch.long)\n",
        "\n",
        "        return {\n",
        "            'image': image,\n",
        "            'input_ids': input_ids,\n",
        "            'attention_mask': attention_mask,\n",
        "            'sentiment': sentiment_class\n",
        "        }\n",
        "\n",
        "\n",
        "# ===================== Gated Multimodal Fusion Module =====================\n",
        "class GatedMultimodalFusion(nn.Module):\n",
        "    \"\"\"\n",
        "    Gated fusion that learns to dynamically weight image and text features.\n",
        "    Uses learnable gates to control modality importance.\n",
        "    \"\"\"\n",
        "    def __init__(self, embed_dim, dropout=0.1):\n",
        "        super(GatedMultimodalFusion, self).__init__()\n",
        "\n",
        "        # Attention-based pooling (better than mean pooling)\n",
        "        self.image_pooling = nn.Sequential(\n",
        "            nn.Linear(embed_dim, embed_dim),\n",
        "            nn.Tanh(),\n",
        "            nn.Linear(embed_dim, 1)\n",
        "        )\n",
        "\n",
        "        self.text_pooling = nn.Sequential(\n",
        "            nn.Linear(embed_dim, embed_dim),\n",
        "            nn.Tanh(),\n",
        "            nn.Linear(embed_dim, 1)\n",
        "        )\n",
        "\n",
        "        # Gate network - learns to balance modalities\n",
        "        self.gate = nn.Sequential(\n",
        "            nn.Linear(embed_dim * 2, embed_dim),\n",
        "            nn.LayerNorm(embed_dim),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(dropout),\n",
        "            nn.Linear(embed_dim, embed_dim),\n",
        "            nn.Sigmoid()  # Output between 0 and 1\n",
        "        )\n",
        "\n",
        "        # Transformation layers for each modality\n",
        "        self.image_transform = nn.Sequential(\n",
        "            nn.Linear(embed_dim, embed_dim),\n",
        "            nn.LayerNorm(embed_dim),\n",
        "            nn.GELU(),\n",
        "            nn.Dropout(dropout)\n",
        "        )\n",
        "\n",
        "        self.text_transform = nn.Sequential(\n",
        "            nn.Linear(embed_dim, embed_dim),\n",
        "            nn.LayerNorm(embed_dim),\n",
        "            nn.GELU(),\n",
        "            nn.Dropout(dropout)\n",
        "        )\n",
        "\n",
        "        # Final fusion layer\n",
        "        self.fusion_layer = nn.Sequential(\n",
        "            nn.Linear(embed_dim * 2, embed_dim * 2),\n",
        "            nn.LayerNorm(embed_dim * 2),\n",
        "            nn.GELU(),\n",
        "            nn.Dropout(dropout)\n",
        "        )\n",
        "\n",
        "    def forward(self, image_feats, text_feats):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            image_feats: [B, seq_i, D] - Image patch features\n",
        "            text_feats: [B, seq_t, D] - Text token features\n",
        "        Returns:\n",
        "            fused_features: [B, 2*D] - Concatenated gated features\n",
        "            image_pooled: [B, D] - Pooled image features\n",
        "            text_pooled: [B, D] - Pooled text features\n",
        "        \"\"\"\n",
        "        # Attention-based pooling\n",
        "        image_weights = F.softmax(self.image_pooling(image_feats), dim=1)  # [B, seq_i, 1]\n",
        "        image_pooled = (image_feats * image_weights).sum(dim=1)  # [B, D]\n",
        "\n",
        "        text_weights = F.softmax(self.text_pooling(text_feats), dim=1)  # [B, seq_t, 1]\n",
        "        text_pooled = (text_feats * text_weights).sum(dim=1)  # [B, D]\n",
        "\n",
        "        # Transform features\n",
        "        image_transformed = self.image_transform(image_pooled)  # [B, D]\n",
        "        text_transformed = self.text_transform(text_pooled)  # [B, D]\n",
        "\n",
        "        # Compute gate values based on both modalities\n",
        "        combined = torch.cat([image_transformed, text_transformed], dim=1)  # [B, 2*D]\n",
        "        gate_values = self.gate(combined)  # [B, D] - values between 0 and 1\n",
        "\n",
        "        # Apply gating: gate controls image importance, (1-gate) controls text importance\n",
        "        gated_image = gate_values * image_transformed  # [B, D]\n",
        "        gated_text = (1 - gate_values) * text_transformed  # [B, D]\n",
        "\n",
        "        # Combine gated features\n",
        "        fused = torch.cat([gated_image, gated_text], dim=1)  # [B, 2*D]\n",
        "        fused_features = self.fusion_layer(fused)  # [B, 2*D]\n",
        "\n",
        "        return fused_features, image_pooled, text_pooled\n",
        "\n",
        "\n",
        "# ===================== Enhanced Loss Function =====================\n",
        "class EnhancedLoss(nn.Module):\n",
        "    def __init__(self, contrastive_weight=0.1, temperature=0.07):\n",
        "        super(EnhancedLoss, self).__init__()\n",
        "        self.ce_loss = nn.CrossEntropyLoss()\n",
        "        self.contrastive_weight = contrastive_weight\n",
        "        self.temperature = temperature\n",
        "\n",
        "    def forward(self, logits, labels, image_feats, text_feats):\n",
        "        # Classification loss\n",
        "        ce_loss = self.ce_loss(logits, labels)\n",
        "\n",
        "        # Contrastive alignment loss (image-text alignment)\n",
        "        image_feats_norm = F.normalize(image_feats, dim=-1)\n",
        "        text_feats_norm = F.normalize(text_feats, dim=-1)\n",
        "\n",
        "        # Cosine similarity matrix\n",
        "        similarity = torch.matmul(image_feats_norm, text_feats_norm.T) / self.temperature\n",
        "\n",
        "        # Contrastive loss (InfoNCE)\n",
        "        batch_size = image_feats.size(0)\n",
        "        labels_contrastive = torch.arange(batch_size).to(image_feats.device)\n",
        "\n",
        "        contrastive_loss = (\n",
        "            F.cross_entropy(similarity, labels_contrastive) +\n",
        "            F.cross_entropy(similarity.T, labels_contrastive)\n",
        "        ) / 2\n",
        "\n",
        "        total_loss = ce_loss + self.contrastive_weight * contrastive_loss\n",
        "\n",
        "        return total_loss, ce_loss, contrastive_loss\n",
        "\n",
        "\n",
        "# ===================== Main Model with Gated Fusion =====================\n",
        "class CustomCLIPBERTModelGated(nn.Module):\n",
        "    \"\"\"\n",
        "    CLIP-BERT model using Gated Multimodal Fusion.\n",
        "    \"\"\"\n",
        "    def __init__(self, clip_model, bert_model, freeze_strategy='freeze_all'):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            freeze_strategy: 'freeze_all', 'freeze_early', or 'train_all'\n",
        "        \"\"\"\n",
        "        super(CustomCLIPBERTModelGated, self).__init__()\n",
        "        self.clip_model = clip_model\n",
        "        self.bert_model = bert_model\n",
        "\n",
        "        # Apply freezing strategy\n",
        "        if freeze_strategy == 'freeze_all':\n",
        "            # RECOMMENDED: Freeze all CLIP and BERT layers\n",
        "            print(\"Freezing ALL CLIP and BERT layers (only training fusion + classifier)\")\n",
        "            for param in self.clip_model.parameters():\n",
        "                param.requires_grad = False\n",
        "            for param in self.bert_model.parameters():\n",
        "                param.requires_grad = False\n",
        "\n",
        "        elif freeze_strategy == 'freeze_early':\n",
        "            # Freeze early layers, train late layers\n",
        "            print(\"Freezing early layers of CLIP and BERT (training top 3 layers + fusion + classifier)\")\n",
        "            # Freeze CLIP early layers\n",
        "            for name, param in self.clip_model.named_parameters():\n",
        "                if 'encoder.layers' in name:\n",
        "                    layer_num = int(name.split('encoder.layers.')[1].split('.')[0])\n",
        "                    if layer_num < 9:  # Freeze first 9 of 12 layers\n",
        "                        param.requires_grad = False\n",
        "                    else:\n",
        "                        param.requires_grad = True\n",
        "                else:\n",
        "                    param.requires_grad = False  # Freeze embeddings\n",
        "\n",
        "            # Freeze BERT early layers\n",
        "            for name, param in self.bert_model.named_parameters():\n",
        "                if 'encoder.layer' in name:\n",
        "                    layer_num = int(name.split('encoder.layer.')[1].split('.')[0])\n",
        "                    if layer_num < 9:  # Freeze first 9 of 12 layers\n",
        "                        param.requires_grad = False\n",
        "                    else:\n",
        "                        param.requires_grad = True\n",
        "                else:\n",
        "                    param.requires_grad = False  # Freeze embeddings\n",
        "\n",
        "        elif freeze_strategy == 'train_all':\n",
        "            # Train everything (NOT RECOMMENDED for small datasets)\n",
        "            print(\"Training ALL layers (WARNING: High overfitting risk on small datasets)\")\n",
        "            for param in self.clip_model.parameters():\n",
        "                param.requires_grad = True\n",
        "            for param in self.bert_model.parameters():\n",
        "                param.requires_grad = True\n",
        "        else:\n",
        "            raise ValueError(f\"Unknown freeze_strategy: {freeze_strategy}\")\n",
        "\n",
        "        # Project CLIP vision features to BERT dimension (768)\n",
        "        self.image_proj = nn.Sequential(\n",
        "            nn.Linear(768, 768),\n",
        "            nn.LayerNorm(768),\n",
        "            nn.GELU()\n",
        "        )\n",
        "\n",
        "        # Gated Multimodal Fusion\n",
        "        self.gated_fusion = GatedMultimodalFusion(embed_dim=768, dropout=0.1)\n",
        "\n",
        "        # Enhanced Classifier head with BatchNorm and higher dropout\n",
        "        self.fc1 = nn.Linear(768 * 2, 512)\n",
        "        self.bn1 = nn.BatchNorm1d(512)\n",
        "        self.dropout1 = nn.Dropout(0.5)\n",
        "\n",
        "        self.fc2 = nn.Linear(512, 256)\n",
        "        self.bn2 = nn.BatchNorm1d(256)\n",
        "        self.dropout2 = nn.Dropout(0.4)\n",
        "\n",
        "        self.fc3 = nn.Linear(256, 128)\n",
        "        self.bn3 = nn.BatchNorm1d(128)\n",
        "        self.dropout3 = nn.Dropout(0.4)\n",
        "\n",
        "        self.fc4 = nn.Linear(128, 3)\n",
        "        self.gelu = nn.GELU()\n",
        "\n",
        "    def forward(self, image, input_ids, attention_mask):\n",
        "        # Extract CLIP vision features (patch embeddings)\n",
        "        vision_outputs = self.clip_model.vision_model(pixel_values=image)\n",
        "        image_features = vision_outputs.last_hidden_state  # [B, 50, 768]\n",
        "        image_features = self.image_proj(image_features)\n",
        "\n",
        "        # Extract BERT text features\n",
        "        bert_output = self.bert_model(\n",
        "            input_ids=input_ids,\n",
        "            attention_mask=attention_mask,\n",
        "            output_hidden_states=True\n",
        "        )\n",
        "        text_features = bert_output.last_hidden_state  # [B, seq_len, 768]\n",
        "\n",
        "        # Gated fusion\n",
        "        fused_features, image_pooled, text_pooled = self.gated_fusion(\n",
        "            image_features, text_features\n",
        "        )\n",
        "\n",
        "        # Classification with BatchNorm\n",
        "        x = self.fc1(fused_features)\n",
        "        x = self.bn1(x)\n",
        "        x = self.gelu(x)\n",
        "        x = self.dropout1(x)\n",
        "\n",
        "        x = self.fc2(x)\n",
        "        x = self.bn2(x)\n",
        "        x = self.gelu(x)\n",
        "        x = self.dropout2(x)\n",
        "\n",
        "        x = self.fc3(x)\n",
        "        x = self.bn3(x)\n",
        "        x = self.gelu(x)\n",
        "        x = self.dropout3(x)\n",
        "\n",
        "        logits = self.fc4(x)\n",
        "\n",
        "        return logits, image_pooled, text_pooled\n",
        "\n",
        "\n",
        "# ===================== Utility Classes =====================\n",
        "class AvgMeter:\n",
        "    def __init__(self):\n",
        "        self.reset()\n",
        "\n",
        "    def reset(self):\n",
        "        self.val = 0\n",
        "        self.avg = 0\n",
        "        self.sum = 0\n",
        "        self.count = 0\n",
        "\n",
        "    def update(self, val, n=1):\n",
        "        self.val = val\n",
        "        self.sum += val * n\n",
        "        self.count += n\n",
        "        self.avg = self.sum / self.count\n",
        "\n",
        "\n",
        "def get_lr(optimizer):\n",
        "    for param_group in optimizer.param_groups:\n",
        "        return param_group['lr']\n",
        "\n",
        "\n",
        "# ===================== Training Function =====================\n",
        "def train_epoch(model, train_loader, optimizer, scheduler, device, criterion, scaler=None):\n",
        "    model.train()\n",
        "    loss_meter = AvgMeter()\n",
        "    ce_loss_meter = AvgMeter()\n",
        "    contrastive_loss_meter = AvgMeter()\n",
        "    correct_predictions = 0\n",
        "    total_predictions = 0\n",
        "\n",
        "    tqdm_object = tqdm(train_loader, total=len(train_loader))\n",
        "\n",
        "    for batch in tqdm_object:\n",
        "        images = batch['image'].to(device)\n",
        "        input_ids = batch['input_ids'].to(device)\n",
        "        attention_mask = batch['attention_mask'].to(device)\n",
        "        sentiments = batch['sentiment'].to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # Mixed precision training\n",
        "        if scaler is not None:\n",
        "            with autocast():\n",
        "                logits, image_feats, text_feats = model(images, input_ids, attention_mask)\n",
        "                loss, ce_loss, contrastive_loss = criterion(\n",
        "                    logits, sentiments, image_feats, text_feats\n",
        "                )\n",
        "\n",
        "            scaler.scale(loss).backward()\n",
        "            scaler.unscale_(optimizer)\n",
        "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
        "            scaler.step(optimizer)\n",
        "            scaler.update()\n",
        "        else:\n",
        "            logits, image_feats, text_feats = model(images, input_ids, attention_mask)\n",
        "            loss, ce_loss, contrastive_loss = criterion(\n",
        "                logits, sentiments, image_feats, text_feats\n",
        "            )\n",
        "            loss.backward()\n",
        "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
        "            optimizer.step()\n",
        "\n",
        "        scheduler.step()\n",
        "\n",
        "        count = images.size(0)\n",
        "        loss_meter.update(loss.item(), count)\n",
        "        ce_loss_meter.update(ce_loss.item(), count)\n",
        "        contrastive_loss_meter.update(contrastive_loss.item(), count)\n",
        "\n",
        "        preds = logits.argmax(dim=1)\n",
        "        correct_predictions += (preds == sentiments).sum().item()\n",
        "        total_predictions += sentiments.size(0)\n",
        "\n",
        "        tqdm_object.set_postfix(\n",
        "            train_loss=loss_meter.avg,\n",
        "            ce_loss=ce_loss_meter.avg,\n",
        "            contrast_loss=contrastive_loss_meter.avg,\n",
        "            lr=get_lr(optimizer)\n",
        "        )\n",
        "\n",
        "    accuracy = correct_predictions / total_predictions\n",
        "    return loss_meter, ce_loss_meter, contrastive_loss_meter, accuracy\n",
        "\n",
        "\n",
        "# ===================== Evaluation Function =====================\n",
        "def evaluate(model, data_loader, device, criterion):\n",
        "    model.eval()\n",
        "    predictions, true_labels = [], []\n",
        "    loss_meter = AvgMeter()\n",
        "    ce_loss_meter = AvgMeter()\n",
        "    contrastive_loss_meter = AvgMeter()\n",
        "    correct_predictions = 0\n",
        "    total_predictions = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch in data_loader:\n",
        "            images = batch['image'].to(device)\n",
        "            input_ids = batch['input_ids'].to(device)\n",
        "            attention_mask = batch['attention_mask'].to(device)\n",
        "            sentiments = batch['sentiment'].to(device)\n",
        "\n",
        "            logits, image_feats, text_feats = model(images, input_ids, attention_mask)\n",
        "            loss, ce_loss, contrastive_loss = criterion(\n",
        "                logits, sentiments, image_feats, text_feats\n",
        "            )\n",
        "\n",
        "            loss_meter.update(loss.item(), len(images))\n",
        "            ce_loss_meter.update(ce_loss.item(), len(images))\n",
        "            contrastive_loss_meter.update(contrastive_loss.item(), len(images))\n",
        "\n",
        "            preds = logits.argmax(dim=1)\n",
        "            correct_predictions += (preds == sentiments).sum().item()\n",
        "            total_predictions += sentiments.size(0)\n",
        "\n",
        "            predictions.extend(preds.cpu().numpy())\n",
        "            true_labels.extend(sentiments.cpu().numpy())\n",
        "\n",
        "    accuracy = correct_predictions / total_predictions\n",
        "    return predictions, true_labels, loss_meter.avg, ce_loss_meter.avg, contrastive_loss_meter.avg, accuracy\n",
        "\n",
        "\n",
        "# ===================== Visualization Functions =====================\n",
        "def plot_training_history(train_losses, val_losses, train_accuracies, val_accuracies):\n",
        "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))\n",
        "\n",
        "    # Loss plot\n",
        "    ax1.plot(train_losses, label='Train Loss', marker='o')\n",
        "    ax1.plot(val_losses, label='Val Loss', marker='s')\n",
        "    ax1.set_xlabel('Epoch')\n",
        "    ax1.set_ylabel('Loss')\n",
        "    ax1.set_title('Training and Validation Loss')\n",
        "    ax1.legend()\n",
        "    ax1.grid(True)\n",
        "\n",
        "    # Accuracy plot\n",
        "    ax2.plot(train_accuracies, label='Train Accuracy', marker='o')\n",
        "    ax2.plot(val_accuracies, label='Val Accuracy', marker='s')\n",
        "    ax2.set_xlabel('Epoch')\n",
        "    ax2.set_ylabel('Accuracy')\n",
        "    ax2.set_title('Training and Validation Accuracy')\n",
        "    ax2.legend()\n",
        "    ax2.grid(True)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.savefig('training_history_gated.png', dpi=300, bbox_inches='tight')\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "def plot_confusion_matrix(true_labels, predictions, class_names=['Negative', 'Neutral', 'Positive']):\n",
        "    cm = confusion_matrix(true_labels, predictions)\n",
        "    plt.figure(figsize=(8, 6))\n",
        "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
        "                xticklabels=class_names, yticklabels=class_names)\n",
        "    plt.title('Confusion Matrix - Gated Fusion')\n",
        "    plt.ylabel('True Label')\n",
        "    plt.xlabel('Predicted Label')\n",
        "    plt.savefig('confusion_matrix_gated.png', dpi=300, bbox_inches='tight')\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "# ===================== Main Training Script =====================\n",
        "# Set device\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "# Data augmentation\n",
        "train_image_transforms = A.Compose([\n",
        "    A.Resize(224, 224),\n",
        "    A.HorizontalFlip(p=0.5),\n",
        "    A.RandomBrightnessContrast(brightness_limit=0.2, contrast_limit=0.2, p=0.3),\n",
        "    A.ShiftScaleRotate(shift_limit=0.1, scale_limit=0.1, rotate_limit=15, p=0.5),\n",
        "    A.CoarseDropout(max_holes=8, max_height=16, max_width=16, p=0.2),\n",
        "    A.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
        "])\n",
        "\n",
        "val_image_transforms = A.Compose([\n",
        "    A.Resize(224, 224),\n",
        "    A.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
        "])\n",
        "\n",
        "# Load models\n",
        "print(\"Loading CLIP and BERT models...\")\n",
        "clip_model = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
        "bert_tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "bert_model = BertModel.from_pretrained('bert-base-uncased')\n",
        "\n",
        "# Create model with GATED FUSION\n",
        "print(\"\\nCreating model with Gated Multimodal Fusion...\")\n",
        "\n",
        "# For 7k dataset with unfrozen layers, use 'freeze_early' as compromise\n",
        "# If you want full training, be prepared for 20-30 epochs\n",
        "model = CustomCLIPBERTModelGated(\n",
        "    clip_model,\n",
        "    bert_model,\n",
        "    freeze_strategy='train_all'  # Changed to train_all based on your decision\n",
        ")\n",
        "model = model.to(device)\n",
        "\n",
        "total_params = sum(p.numel() for p in model.parameters())\n",
        "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "\n",
        "print(f\"\\nModel Statistics:\")\n",
        "print(f\"  Total parameters:     {total_params:,}\")\n",
        "print(f\"  Trainable parameters: {trainable_params:,} ({trainable_params/total_params*100:.2f}%)\")\n",
        "print(f\"  Frozen parameters:    {total_params-trainable_params:,} ({(total_params-trainable_params)/total_params*100:.2f}%)\")\n",
        "\n",
        "# Warning for small datasets\n",
        "if trainable_params > 50_000_000:  # 50M+ trainable params\n",
        "    print(f\"\\n⚠️  WARNING: Training {trainable_params:,} parameters on 7k samples\")\n",
        "    print(f\"   Recommendation: Use 20-30 epochs or consider 'freeze_early' strategy\")\n",
        "\n",
        "# Dataset paths (update these to your actual paths)\n",
        "train_image_dir = '/content/drive/MyDrive/memo_3/trainImages/trainImages'\n",
        "val_image_dir = '/content/drive/MyDrive/memo_3/valImages/valImages'\n",
        "\n",
        "# Create datasets - REPLACE train_df and val_df with your actual DataFrames\n",
        "train_dataset = MemeDataset(\n",
        "    images=train_df['image_url'].tolist(),\n",
        "    captions=train_df['ocr'].tolist(),\n",
        "    sentiments=train_df['overall'].tolist(),\n",
        "    tokenizer=bert_tokenizer,\n",
        "    image_transforms=train_image_transforms,\n",
        "    image_dir=train_image_dir\n",
        ")\n",
        "\n",
        "val_dataset = MemeDataset(\n",
        "    images=val_df['image_url'].tolist(),\n",
        "    captions=val_df['ocr'].tolist(),\n",
        "    sentiments=val_df['overall'].tolist(),\n",
        "    tokenizer=bert_tokenizer,\n",
        "    image_transforms=val_image_transforms,\n",
        "    image_dir=val_image_dir\n",
        ")\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True, num_workers=4)\n",
        "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False, num_workers=4)\n",
        "\n",
        "# Discriminative learning rates (adjusted for unfrozen layers)\n",
        "# Use VERY LOW learning rates for pretrained models to prevent catastrophic forgetting\n",
        "trainable_params = []\n",
        "\n",
        "# CLIP parameters - VERY LOW LR to preserve pretrained knowledge\n",
        "clip_params = [p for p in model.clip_model.parameters() if p.requires_grad]\n",
        "if clip_params:\n",
        "    trainable_params.append({'params': clip_params, 'lr': 5e-7})  # Reduced from 1e-6\n",
        "\n",
        "# BERT parameters - VERY LOW LR to preserve pretrained knowledge\n",
        "bert_params = [p for p in model.bert_model.parameters() if p.requires_grad]\n",
        "if bert_params:\n",
        "    trainable_params.append({'params': bert_params, 'lr': 5e-7})  # Reduced from 1e-6\n",
        "\n",
        "# New layers can have higher LR\n",
        "trainable_params.extend([\n",
        "    {'params': model.image_proj.parameters(), 'lr': 1e-4},\n",
        "    {'params': model.gated_fusion.parameters(), 'lr': 1e-4},\n",
        "    # Classifier layers (individual fc layers, bn layers, etc.)\n",
        "    {'params': model.fc1.parameters(), 'lr': 1e-4},\n",
        "    {'params': model.bn1.parameters(), 'lr': 1e-4},\n",
        "    {'params': model.fc2.parameters(), 'lr': 1e-4},\n",
        "    {'params': model.bn2.parameters(), 'lr': 1e-4},\n",
        "    {'params': model.fc3.parameters(), 'lr': 1e-4},\n",
        "    {'params': model.bn3.parameters(), 'lr': 1e-4},\n",
        "    {'params': model.fc4.parameters(), 'lr': 1e-4},\n",
        "])\n",
        "\n",
        "optimizer = torch.optim.AdamW(trainable_params, weight_decay=1e-4)\n",
        "\n",
        "# INCREASED epochs for full training (was 10, now 25)\n",
        "num_epochs = 25\n",
        "\n",
        "print(f\"\\nTraining Configuration:\")\n",
        "print(f\"  Number of epochs: {num_epochs}\")\n",
        "print(f\"  Batch size: 32\")\n",
        "print(f\"  Total training steps: {len(train_loader) * num_epochs:,}\")\n",
        "print(f\"  Warmup steps: {len(train_loader) * 3:,} (3 epochs)\")\n",
        "\n",
        "scheduler = get_linear_schedule_with_warmup(\n",
        "    optimizer,\n",
        "    num_warmup_steps=len(train_loader) * 3,  # 3 epochs warmup (was 2)\n",
        "    num_training_steps=len(train_loader) * num_epochs\n",
        ")\n",
        "\n",
        "# Loss function\n",
        "criterion = EnhancedLoss(contrastive_weight=0.1, temperature=0.07)\n",
        "\n",
        "# Mixed precision training\n",
        "scaler = GradScaler() if device == 'cuda' else None\n",
        "\n",
        "# Training tracking\n",
        "BEST_MODEL_PATH = 'best_model_gated.pth'\n",
        "train_losses = []\n",
        "val_losses = []\n",
        "train_accuracies = []\n",
        "val_accuracies = []\n",
        "best_val_accuracy = 0\n",
        "patience = 5  # Increased from 3 for longer training\n",
        "patience_counter = 0\n",
        "\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"STARTING TRAINING WITH GATED MULTIMODAL FUSION\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    print(f\"\\n{'='*50}\")\n",
        "    print(f\"Epoch {epoch + 1}/{num_epochs}\")\n",
        "    print(f\"{'='*50}\")\n",
        "\n",
        "    # Train\n",
        "    train_loss, train_ce, train_contrast, train_accuracy = train_epoch(\n",
        "        model, train_loader, optimizer, scheduler, device, criterion, scaler\n",
        "    )\n",
        "\n",
        "    # Validate\n",
        "    val_predictions, val_true_labels, val_loss, val_ce, val_contrast, val_accuracy = evaluate(\n",
        "        model, val_loader, device, criterion\n",
        "    )\n",
        "\n",
        "    # Save best model\n",
        "    if val_accuracy > best_val_accuracy:\n",
        "        best_val_accuracy = val_accuracy\n",
        "        torch.save({\n",
        "            'epoch': epoch,\n",
        "            'model_state_dict': model.state_dict(),\n",
        "            'optimizer_state_dict': optimizer.state_dict(),\n",
        "            'val_accuracy': val_accuracy,\n",
        "        }, BEST_MODEL_PATH)\n",
        "        print(f\"✓ Best model saved with validation accuracy: {val_accuracy:.4f}\")\n",
        "        patience_counter = 0\n",
        "    else:\n",
        "        patience_counter += 1\n",
        "\n",
        "    # Store metrics\n",
        "    train_losses.append(train_loss.avg)\n",
        "    val_losses.append(val_loss)\n",
        "    train_accuracies.append(train_accuracy)\n",
        "    val_accuracies.append(val_accuracy)\n",
        "\n",
        "    # Print epoch summary\n",
        "    print(f\"\\nEpoch {epoch + 1} Summary:\")\n",
        "    print(f\"  Train Loss: {train_loss.avg:.4f} (CE: {train_ce.avg:.4f}, Contrast: {train_contrast.avg:.4f})\")\n",
        "    print(f\"  Train Accuracy: {train_accuracy:.4f}\")\n",
        "    print(f\"  Val Loss: {val_loss:.4f} (CE: {val_ce:.4f}, Contrast: {val_contrast:.4f})\")\n",
        "    print(f\"  Val Accuracy: {val_accuracy:.4f}\")\n",
        "    print(f\"  Best Val Accuracy: {best_val_accuracy:.4f}\")\n",
        "\n",
        "    # Early stopping\n",
        "    # if patience_counter >= patience:\n",
        "    #     print(f\"\\nEarly stopping triggered after {epoch + 1} epochs\")\n",
        "    #     break\n",
        "\n",
        "# Load best model\n",
        "print(f\"\\nLoading best model with validation accuracy: {best_val_accuracy:.4f}\")\n",
        "checkpoint = torch.load(BEST_MODEL_PATH)\n",
        "model.load_state_dict(checkpoint['model_state_dict'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AbqxSJTZFX_X",
        "outputId": "b5a777d1-2071-493c-95db-8412d4d60b8d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n",
            "Loading CLIP and BERT models...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/albumentations/core/validation.py:114: UserWarning: ShiftScaleRotate is a special case of Affine transform. Please use Affine transform instead.\n",
            "  original_init(self, **validated_kwargs)\n",
            "/tmp/ipython-input-1956123676.py:482: UserWarning: Argument(s) 'max_holes, max_height, max_width' are not valid for transform CoarseDropout\n",
            "  A.CoarseDropout(max_holes=8, max_height=16, max_width=16, p=0.2),\n",
            "/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Creating model with Gated Multimodal Fusion...\n",
            "Training ALL layers (WARNING: High overfitting risk on small datasets)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/tmp/ipython-input-1956123676.py:597: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
            "  scaler = GradScaler() if device == 'cuda' else None\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Model Statistics:\n",
            "  Total parameters:     268,808,454\n",
            "  Trainable parameters: 268,808,454 (100.00%)\n",
            "  Frozen parameters:    0 (0.00%)\n",
            "\n",
            "⚠️  WARNING: Training 268,808,454 parameters on 7k samples\n",
            "   Recommendation: Use 20-30 epochs or consider 'freeze_early' strategy\n",
            "\n",
            "Training Configuration:\n",
            "  Number of epochs: 25\n",
            "  Batch size: 32\n",
            "  Total training steps: 5,475\n",
            "  Warmup steps: 657 (3 epochs)\n",
            "\n",
            "======================================================================\n",
            "STARTING TRAINING WITH GATED MULTIMODAL FUSION\n",
            "======================================================================\n",
            "\n",
            "==================================================\n",
            "Epoch 1/25\n",
            "==================================================\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  0%|          | 0/219 [00:00<?, ?it/s]/tmp/ipython-input-1956123676.py:353: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with autocast():\n",
            "100%|██████████| 219/219 [01:06<00:00,  3.29it/s, ce_loss=1.18, contrast_loss=3.48, lr=1.67e-7, train_loss=1.53]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✓ Best model saved with validation accuracy: 0.2568\n",
            "\n",
            "Epoch 1 Summary:\n",
            "  Train Loss: 1.5263 (CE: 1.1783, Contrast: 3.4794)\n",
            "  Train Accuracy: 0.3179\n",
            "  Val Loss: 1.4680 (CE: 1.1206, Contrast: 3.4741)\n",
            "  Val Accuracy: 0.2568\n",
            "  Best Val Accuracy: 0.2568\n",
            "\n",
            "==================================================\n",
            "Epoch 2/25\n",
            "==================================================\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 219/219 [01:09<00:00,  3.16it/s, ce_loss=1.14, contrast_loss=3.48, lr=3.33e-7, train_loss=1.48]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✓ Best model saved with validation accuracy: 0.3069\n",
            "\n",
            "Epoch 2 Summary:\n",
            "  Train Loss: 1.4837 (CE: 1.1362, Contrast: 3.4750)\n",
            "  Train Accuracy: 0.3593\n",
            "  Val Loss: 1.4594 (CE: 1.1122, Contrast: 3.4717)\n",
            "  Val Accuracy: 0.3069\n",
            "  Best Val Accuracy: 0.3069\n",
            "\n",
            "==================================================\n",
            "Epoch 3/25\n",
            "==================================================\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 219/219 [01:13<00:00,  2.96it/s, ce_loss=1.11, contrast_loss=3.47, lr=5e-7, train_loss=1.46]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✓ Best model saved with validation accuracy: 0.3389\n",
            "\n",
            "Epoch 3 Summary:\n",
            "  Train Loss: 1.4588 (CE: 1.1117, Contrast: 3.4715)\n",
            "  Train Accuracy: 0.3883\n",
            "  Val Loss: 1.4541 (CE: 1.1070, Contrast: 3.4706)\n",
            "  Val Accuracy: 0.3389\n",
            "  Best Val Accuracy: 0.3389\n",
            "\n",
            "==================================================\n",
            "Epoch 4/25\n",
            "==================================================\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 219/219 [01:13<00:00,  2.98it/s, ce_loss=1.1, contrast_loss=3.47, lr=4.77e-7, train_loss=1.45]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✓ Best model saved with validation accuracy: 0.3562\n",
            "\n",
            "Epoch 4 Summary:\n",
            "  Train Loss: 1.4471 (CE: 1.0997, Contrast: 3.4738)\n",
            "  Train Accuracy: 0.3953\n",
            "  Val Loss: 1.4791 (CE: 1.1320, Contrast: 3.4709)\n",
            "  Val Accuracy: 0.3562\n",
            "  Best Val Accuracy: 0.3562\n",
            "\n",
            "==================================================\n",
            "Epoch 5/25\n",
            "==================================================\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 219/219 [01:06<00:00,  3.28it/s, ce_loss=1.08, contrast_loss=3.47, lr=4.55e-7, train_loss=1.43]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 5 Summary:\n",
            "  Train Loss: 1.4298 (CE: 1.0825, Contrast: 3.4733)\n",
            "  Train Accuracy: 0.4173\n",
            "  Val Loss: 1.4899 (CE: 1.1428, Contrast: 3.4709)\n",
            "  Val Accuracy: 0.3489\n",
            "  Best Val Accuracy: 0.3562\n",
            "\n",
            "==================================================\n",
            "Epoch 6/25\n",
            "==================================================\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 219/219 [01:06<00:00,  3.27it/s, ce_loss=1.06, contrast_loss=3.47, lr=4.32e-7, train_loss=1.41]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 6 Summary:\n",
            "  Train Loss: 1.4122 (CE: 1.0649, Contrast: 3.4731)\n",
            "  Train Accuracy: 0.4317\n",
            "  Val Loss: 1.5031 (CE: 1.1561, Contrast: 3.4700)\n",
            "  Val Accuracy: 0.3502\n",
            "  Best Val Accuracy: 0.3562\n",
            "\n",
            "==================================================\n",
            "Epoch 7/25\n",
            "==================================================\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 219/219 [01:06<00:00,  3.29it/s, ce_loss=1.05, contrast_loss=3.47, lr=4.09e-7, train_loss=1.4]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 7 Summary:\n",
            "  Train Loss: 1.3950 (CE: 1.0478, Contrast: 3.4724)\n",
            "  Train Accuracy: 0.4476\n",
            "  Val Loss: 1.4954 (CE: 1.1483, Contrast: 3.4710)\n",
            "  Val Accuracy: 0.3542\n",
            "  Best Val Accuracy: 0.3562\n",
            "\n",
            "==================================================\n",
            "Epoch 8/25\n",
            "==================================================\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 219/219 [01:06<00:00,  3.28it/s, ce_loss=1.04, contrast_loss=3.47, lr=3.86e-7, train_loss=1.39]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 8 Summary:\n",
            "  Train Loss: 1.3862 (CE: 1.0390, Contrast: 3.4723)\n",
            "  Train Accuracy: 0.4620\n",
            "  Val Loss: 1.5207 (CE: 1.1737, Contrast: 3.4693)\n",
            "  Val Accuracy: 0.3556\n",
            "  Best Val Accuracy: 0.3562\n",
            "\n",
            "==================================================\n",
            "Epoch 9/25\n",
            "==================================================\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 219/219 [01:10<00:00,  3.09it/s, ce_loss=1.02, contrast_loss=3.47, lr=3.64e-7, train_loss=1.37]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 9 Summary:\n",
            "  Train Loss: 1.3715 (CE: 1.0243, Contrast: 3.4721)\n",
            "  Train Accuracy: 0.4761\n",
            "  Val Loss: 1.5207 (CE: 1.1737, Contrast: 3.4698)\n",
            "  Val Accuracy: 0.3562\n",
            "  Best Val Accuracy: 0.3562\n",
            "\n",
            "==================================================\n",
            "Epoch 10/25\n",
            "==================================================\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 219/219 [01:06<00:00,  3.32it/s, ce_loss=1.01, contrast_loss=3.47, lr=3.41e-7, train_loss=1.36]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 10 Summary:\n",
            "  Train Loss: 1.3572 (CE: 1.0100, Contrast: 3.4721)\n",
            "  Train Accuracy: 0.4903\n",
            "  Val Loss: 1.5362 (CE: 1.1893, Contrast: 3.4688)\n",
            "  Val Accuracy: 0.3442\n",
            "  Best Val Accuracy: 0.3562\n",
            "\n",
            "==================================================\n",
            "Epoch 11/25\n",
            "==================================================\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 219/219 [01:09<00:00,  3.14it/s, ce_loss=0.994, contrast_loss=3.47, lr=3.18e-7, train_loss=1.34]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 11 Summary:\n",
            "  Train Loss: 1.3409 (CE: 0.9938, Contrast: 3.4712)\n",
            "  Train Accuracy: 0.5007\n",
            "  Val Loss: 1.5494 (CE: 1.2025, Contrast: 3.4689)\n",
            "  Val Accuracy: 0.3436\n",
            "  Best Val Accuracy: 0.3562\n",
            "\n",
            "==================================================\n",
            "Epoch 12/25\n",
            "==================================================\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 219/219 [01:06<00:00,  3.28it/s, ce_loss=0.976, contrast_loss=3.47, lr=2.95e-7, train_loss=1.32]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 12 Summary:\n",
            "  Train Loss: 1.3231 (CE: 0.9761, Contrast: 3.4703)\n",
            "  Train Accuracy: 0.5139\n",
            "  Val Loss: 1.5446 (CE: 1.1977, Contrast: 3.4684)\n",
            "  Val Accuracy: 0.3476\n",
            "  Best Val Accuracy: 0.3562\n",
            "\n",
            "==================================================\n",
            "Epoch 13/25\n",
            "==================================================\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 219/219 [01:06<00:00,  3.30it/s, ce_loss=0.969, contrast_loss=3.47, lr=2.73e-7, train_loss=1.32]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 13 Summary:\n",
            "  Train Loss: 1.3159 (CE: 0.9688, Contrast: 3.4705)\n",
            "  Train Accuracy: 0.5221\n",
            "  Val Loss: 1.5636 (CE: 1.2168, Contrast: 3.4682)\n",
            "  Val Accuracy: 0.3522\n",
            "  Best Val Accuracy: 0.3562\n",
            "\n",
            "==================================================\n",
            "Epoch 14/25\n",
            "==================================================\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 219/219 [01:05<00:00,  3.32it/s, ce_loss=0.951, contrast_loss=3.47, lr=2.5e-7, train_loss=1.3]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 14 Summary:\n",
            "  Train Loss: 1.2983 (CE: 0.9511, Contrast: 3.4712)\n",
            "  Train Accuracy: 0.5386\n",
            "  Val Loss: 1.5840 (CE: 1.2372, Contrast: 3.4682)\n",
            "  Val Accuracy: 0.3522\n",
            "  Best Val Accuracy: 0.3562\n",
            "\n",
            "==================================================\n",
            "Epoch 15/25\n",
            "==================================================\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 219/219 [01:07<00:00,  3.24it/s, ce_loss=0.926, contrast_loss=3.47, lr=2.27e-7, train_loss=1.27]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 15 Summary:\n",
            "  Train Loss: 1.2730 (CE: 0.9259, Contrast: 3.4704)\n",
            "  Train Accuracy: 0.5481\n",
            "  Val Loss: 1.6151 (CE: 1.2683, Contrast: 3.4681)\n",
            "  Val Accuracy: 0.3396\n",
            "  Best Val Accuracy: 0.3562\n",
            "\n",
            "==================================================\n",
            "Epoch 16/25\n",
            "==================================================\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 219/219 [01:49<00:00,  2.00it/s, ce_loss=0.9, contrast_loss=3.47, lr=2.05e-7, train_loss=1.25]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 16 Summary:\n",
            "  Train Loss: 1.2471 (CE: 0.9001, Contrast: 3.4702)\n",
            "  Train Accuracy: 0.5784\n",
            "  Val Loss: 1.6170 (CE: 1.2702, Contrast: 3.4679)\n",
            "  Val Accuracy: 0.3529\n",
            "  Best Val Accuracy: 0.3562\n",
            "\n",
            "==================================================\n",
            "Epoch 17/25\n",
            "==================================================\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 219/219 [01:08<00:00,  3.17it/s, ce_loss=0.867, contrast_loss=3.47, lr=1.82e-7, train_loss=1.21]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 17 Summary:\n",
            "  Train Loss: 1.2138 (CE: 0.8668, Contrast: 3.4700)\n",
            "  Train Accuracy: 0.5914\n",
            "  Val Loss: 1.6455 (CE: 1.2987, Contrast: 3.4679)\n",
            "  Val Accuracy: 0.3322\n",
            "  Best Val Accuracy: 0.3562\n",
            "\n",
            "==================================================\n",
            "Epoch 18/25\n",
            "==================================================\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 219/219 [01:07<00:00,  3.27it/s, ce_loss=0.838, contrast_loss=3.47, lr=1.59e-7, train_loss=1.18]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 18 Summary:\n",
            "  Train Loss: 1.1850 (CE: 0.8380, Contrast: 3.4699)\n",
            "  Train Accuracy: 0.6176\n",
            "  Val Loss: 1.6921 (CE: 1.3454, Contrast: 3.4673)\n",
            "  Val Accuracy: 0.3329\n",
            "  Best Val Accuracy: 0.3562\n",
            "\n",
            "==================================================\n",
            "Epoch 19/25\n",
            "==================================================\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 219/219 [01:06<00:00,  3.31it/s, ce_loss=0.817, contrast_loss=3.47, lr=1.36e-7, train_loss=1.16]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✓ Best model saved with validation accuracy: 0.3596\n",
            "\n",
            "Epoch 19 Summary:\n",
            "  Train Loss: 1.1636 (CE: 0.8166, Contrast: 3.4698)\n",
            "  Train Accuracy: 0.6357\n",
            "  Val Loss: 1.6530 (CE: 1.3062, Contrast: 3.4673)\n",
            "  Val Accuracy: 0.3596\n",
            "  Best Val Accuracy: 0.3596\n",
            "\n",
            "==================================================\n",
            "Epoch 20/25\n",
            "==================================================\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 219/219 [01:30<00:00,  2.41it/s, ce_loss=0.795, contrast_loss=3.47, lr=1.14e-7, train_loss=1.14]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 20 Summary:\n",
            "  Train Loss: 1.1417 (CE: 0.7947, Contrast: 3.4700)\n",
            "  Train Accuracy: 0.6426\n",
            "  Val Loss: 1.7064 (CE: 1.3597, Contrast: 3.4673)\n",
            "  Val Accuracy: 0.3336\n",
            "  Best Val Accuracy: 0.3596\n",
            "\n",
            "==================================================\n",
            "Epoch 21/25\n",
            "==================================================\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 219/219 [01:07<00:00,  3.26it/s, ce_loss=0.768, contrast_loss=3.47, lr=9.09e-8, train_loss=1.11]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 21 Summary:\n",
            "  Train Loss: 1.1148 (CE: 0.7679, Contrast: 3.4694)\n",
            "  Train Accuracy: 0.6611\n",
            "  Val Loss: 1.7166 (CE: 1.3698, Contrast: 3.4674)\n",
            "  Val Accuracy: 0.3449\n",
            "  Best Val Accuracy: 0.3596\n",
            "\n",
            "==================================================\n",
            "Epoch 22/25\n",
            "==================================================\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 219/219 [01:06<00:00,  3.29it/s, ce_loss=0.746, contrast_loss=3.47, lr=6.82e-8, train_loss=1.09]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 22 Summary:\n",
            "  Train Loss: 1.0929 (CE: 0.7460, Contrast: 3.4694)\n",
            "  Train Accuracy: 0.6780\n",
            "  Val Loss: 1.7110 (CE: 1.3643, Contrast: 3.4672)\n",
            "  Val Accuracy: 0.3469\n",
            "  Best Val Accuracy: 0.3596\n",
            "\n",
            "==================================================\n",
            "Epoch 23/25\n",
            "==================================================\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 219/219 [01:07<00:00,  3.25it/s, ce_loss=0.72, contrast_loss=3.47, lr=4.55e-8, train_loss=1.07]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 23 Summary:\n",
            "  Train Loss: 1.0672 (CE: 0.7203, Contrast: 3.4696)\n",
            "  Train Accuracy: 0.6880\n",
            "  Val Loss: 1.7501 (CE: 1.4034, Contrast: 3.4672)\n",
            "  Val Accuracy: 0.3442\n",
            "  Best Val Accuracy: 0.3596\n",
            "\n",
            "==================================================\n",
            "Epoch 24/25\n",
            "==================================================\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 219/219 [01:06<00:00,  3.31it/s, ce_loss=0.708, contrast_loss=3.47, lr=2.27e-8, train_loss=1.05]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 24 Summary:\n",
            "  Train Loss: 1.0548 (CE: 0.7078, Contrast: 3.4697)\n",
            "  Train Accuracy: 0.7017\n",
            "  Val Loss: 1.7432 (CE: 1.3964, Contrast: 3.4672)\n",
            "  Val Accuracy: 0.3449\n",
            "  Best Val Accuracy: 0.3596\n",
            "\n",
            "==================================================\n",
            "Epoch 25/25\n",
            "==================================================\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 219/219 [01:06<00:00,  3.28it/s, ce_loss=0.688, contrast_loss=3.47, lr=0, train_loss=1.03]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 25 Summary:\n",
            "  Train Loss: 1.0349 (CE: 0.6880, Contrast: 3.4695)\n",
            "  Train Accuracy: 0.7111\n",
            "  Val Loss: 1.7490 (CE: 1.4023, Contrast: 3.4672)\n",
            "  Val Accuracy: 0.3482\n",
            "  Best Val Accuracy: 0.3596\n",
            "\n",
            "Loading best model with validation accuracy: 0.3596\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Final evaluation\n",
        "print(\"\\nFinal evaluation on validation set:\")\n",
        "val_predictions, val_true_labels, _, _, _, final_accuracy = evaluate(\n",
        "    model, val_loader, device, criterion\n",
        ")\n",
        "\n",
        "# Calculate F1 scores\n",
        "f1_macro = f1_score(val_true_labels, val_predictions, average='macro')\n",
        "f1_weighted = f1_score(val_true_labels, val_predictions, average='weighted')\n",
        "\n",
        "print(f\"\\n{'='*70}\")\n",
        "print(\"FINAL RESULTS - GATED MULTIMODAL FUSION\")\n",
        "print(f\"{'='*70}\")\n",
        "print(f\"Final Validation Accuracy: {final_accuracy:.4f}\")\n",
        "print(f\"F1-Score (Macro):          {f1_macro:.4f}\")\n",
        "print(f\"F1-Score (Weighted):       {f1_weighted:.4f}\")\n",
        "print(f\"{'='*70}\")\n",
        "\n",
        "print(\"\\nClassification Report:\")\n",
        "print(classification_report(\n",
        "    val_true_labels,\n",
        "    val_predictions,\n",
        "    target_names=['Negative', 'Neutral', 'Positive'],\n",
        "    digits=4\n",
        "))\n",
        "\n",
        "# Plot results\n",
        "print(\"\\nGenerating visualizations...\")\n",
        "# plot_training_history(train_losses, val_losses, train_accuracies, val_accuracies)\n",
        "# plot_confusion_matrix(val_true_labels, val_predictions)\n",
        "\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"TRAINING COMPLETE!\")\n",
        "print(\"=\"*70)\n",
        "print(f\"Best Model Path: {BEST_MODEL_PATH}\")\n",
        "print(f\"Best Validation Accuracy: {best_val_accuracy:.4f}\")\n",
        "print(\"Visualizations saved: training_history_gated.png, confusion_matrix_gated.png\")\n",
        "print(\"=\"*70)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m82k11w0FjNb",
        "outputId": "5be86530-1a6d-445c-c891-f92cb248ecad"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Final evaluation on validation set:\n",
            "\n",
            "======================================================================\n",
            "FINAL RESULTS - GATED MULTIMODAL FUSION\n",
            "======================================================================\n",
            "Final Validation Accuracy: 0.3596\n",
            "F1-Score (Macro):          0.3178\n",
            "F1-Score (Weighted):       0.3349\n",
            "======================================================================\n",
            "\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "    Negative     0.4082    0.2069    0.2746       580\n",
            "     Neutral     0.3793    0.6131    0.4686       579\n",
            "    Positive     0.2379    0.1882    0.2102       340\n",
            "\n",
            "    accuracy                         0.3596      1499\n",
            "   macro avg     0.3418    0.3361    0.3178      1499\n",
            "weighted avg     0.3584    0.3596    0.3349      1499\n",
            "\n",
            "\n",
            "Generating visualizations...\n",
            "\n",
            "======================================================================\n",
            "TRAINING COMPLETE!\n",
            "======================================================================\n",
            "Best Model Path: best_model_gated.pth\n",
            "Best Validation Accuracy: 0.3596\n",
            "Visualizations saved: training_history_gated.png, confusion_matrix_gated.png\n",
            "======================================================================\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "plt.plot(train_accuracies)\n",
        "plt.plot(val_accuracies)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 447
        },
        "id": "BW9Ve3BlfhDr",
        "outputId": "e554219f-4be9-48bc-ce4e-66a7cc0a8cc1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[<matplotlib.lines.Line2D at 0x7b9c79172930>]"
            ]
          },
          "metadata": {},
          "execution_count": 16
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    }
  ]
}