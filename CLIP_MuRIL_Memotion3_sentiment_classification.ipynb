{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "mount_file_id": "1aALGaGC2ZCfaVn_p1v9-wOBevvQTIc9V",
      "authorship_tag": "ABX9TyP4e3OpZ7gkIFqj3Ixg6g1W"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# !pip install text_hammer"
      ],
      "metadata": {
        "id": "OsMW1D1tuDWn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.cuda.amp import GradScaler"
      ],
      "metadata": {
        "id": "PLdB5x2jxRwu"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "lDS2g8vgtaEr"
      },
      "outputs": [],
      "source": [
        "\n",
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from transformers import (\n",
        "    CLIPModel,\n",
        "    AutoModel,  # For MuRIL\n",
        "    AutoTokenizer,  # For MuRIL\n",
        "    get_linear_schedule_with_warmup\n",
        ")\n",
        "import albumentations as A\n",
        "from PIL import Image\n",
        "from tqdm import tqdm\n",
        "from sklearn.metrics import classification_report, confusion_matrix, f1_score\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "from torch.cuda.amp import autocast, GradScaler"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_df = pd.read_csv('/content/drive/MyDrive/memo_3/memotion3/memotion3/train.csv')"
      ],
      "metadata": {
        "id": "a5KG3idqtlSh"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_df['overall'] = train_df['overall'].replace({\n",
        "                                                            'very_positive': 2,\n",
        "                                                            'positive': 2,\n",
        "                                                            'neutral': 1,\n",
        "                                                            'very_negative': 0,\n",
        "                                                            'negative': 0})"
      ],
      "metadata": {
        "id": "ivreOgASt5Fa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "val_df = pd.read_csv('/content/drive/MyDrive/memo_3/val.csv')"
      ],
      "metadata": {
        "id": "wc9Nxi7Dt8X1"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "val_df['overall'] = val_df['overall'].replace({\n",
        "                                                            'very_positive': 2,\n",
        "                                                            'positive': 2,\n",
        "                                                            'neutral': 1,\n",
        "                                                            'very_negative': 0,\n",
        "                                                            'negative': 0})"
      ],
      "metadata": {
        "id": "ySgigJ_St-p9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import text_hammer as th"
      ],
      "metadata": {
        "id": "UASrryziuA7B"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "\n",
        "from tqdm._tqdm_notebook import tqdm_notebook\n",
        "tqdm_notebook.pandas()\n",
        "\n",
        "def text_preprocessing(df, col_name):\n",
        "  column = col_name\n",
        "  df[column] = df[column].progress_apply(lambda x:str(x).lower())\n",
        "  df[column] = df[column].progress_apply(lambda x: th.cont_exp(x)) # you're -> you are; we'll be -> we will be\n",
        "  df[column] = df[column].progress_apply(lambda x: th.remove_emails(x))\n",
        "  df[column] = df[column].progress_apply(lambda x: th.remove_html_tags(x))\n",
        "\n",
        "  df[column] = df[column].progress_apply(lambda x: th.remove_special_chars(x))\n",
        "  df[column] = df[column].progress_apply(lambda x: th.remove_accented_chars(x))\n",
        "\n",
        "  return df"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yIn0VfOpuLJa",
        "outputId": "3a035e26-e175-4fd2-d531-0aa0a2a23e5c"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CPU times: user 390 Âµs, sys: 0 ns, total: 390 Âµs\n",
            "Wall time: 371 Âµs\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset = text_preprocessing(train_df, 'ocr')\n",
        "val_dataset = text_preprocessing(val_df, 'ocr')"
      ],
      "metadata": {
        "id": "ACEuNCOLuP0W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ===================== Dataset Class =====================\n",
        "class MemeDataset(Dataset):\n",
        "    def __init__(self, images, captions, sentiments, tokenizer, image_transforms, image_dir):\n",
        "        self.images = images\n",
        "        self.captions = captions\n",
        "        self.sentiments = sentiments\n",
        "        self.tokenizer = tokenizer\n",
        "        self.image_transforms = image_transforms\n",
        "        self.image_dir = image_dir\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.images)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        image_name = self.images[idx]\n",
        "        image_path = os.path.join(self.image_dir, image_name)\n",
        "        caption = self.captions[idx]\n",
        "        sentiment = self.sentiments[idx]\n",
        "\n",
        "        # Load and preprocess image\n",
        "        try:\n",
        "            image = Image.open(image_path).convert('RGB')\n",
        "            image = np.array(image)\n",
        "        except Exception as e:\n",
        "            image = np.full((224, 224, 3), 128, dtype=np.uint8)\n",
        "\n",
        "        # Apply transforms\n",
        "        image = self.image_transforms(image=image)['image']\n",
        "        image = torch.tensor(image).permute(2, 0, 1).float()\n",
        "\n",
        "        # Ensure caption is a valid string\n",
        "        if not isinstance(caption, str):\n",
        "            caption = str(caption) if caption else \"empty caption\"\n",
        "        if isinstance(caption, list):\n",
        "            caption = ' '.join(caption)\n",
        "        if not caption or caption.strip() == '':\n",
        "            caption = \"empty caption\"\n",
        "\n",
        "        # Encode caption with MuRIL tokenizer\n",
        "        # MuRIL uses max_length=512 by default\n",
        "        encoded_caption = self.tokenizer(\n",
        "            caption,\n",
        "            return_tensors=\"pt\",\n",
        "            padding='max_length',\n",
        "            truncation=True,\n",
        "            max_length=128  # Good for Hinglish captions\n",
        "        )\n",
        "        input_ids = encoded_caption['input_ids'].squeeze()\n",
        "        attention_mask = encoded_caption['attention_mask'].squeeze()\n",
        "\n",
        "        sentiment_class = torch.tensor(sentiment, dtype=torch.long)\n",
        "\n",
        "        return {\n",
        "            'image': image,\n",
        "            'input_ids': input_ids,\n",
        "            'attention_mask': attention_mask,\n",
        "            'sentiment': sentiment_class\n",
        "        }\n",
        "\n",
        "\n",
        "# ===================== Cross-Attention Fusion Module =====================\n",
        "class CrossAttentionFusion(nn.Module):\n",
        "    def __init__(self, embed_dim, num_heads=8, dropout=0.1):\n",
        "        super(CrossAttentionFusion, self).__init__()\n",
        "\n",
        "        # Bidirectional cross-attention\n",
        "        self.text_to_image = nn.MultiheadAttention(\n",
        "            embed_dim, num_heads, dropout=dropout, batch_first=True\n",
        "        )\n",
        "        self.image_to_text = nn.MultiheadAttention(\n",
        "            embed_dim, num_heads, dropout=dropout, batch_first=True\n",
        "        )\n",
        "\n",
        "        # Feed-forward networks\n",
        "        self.ffn_text = nn.Sequential(\n",
        "            nn.Linear(embed_dim, embed_dim * 4),\n",
        "            nn.GELU(),\n",
        "            nn.Dropout(dropout),\n",
        "            nn.Linear(embed_dim * 4, embed_dim),\n",
        "            nn.Dropout(dropout)\n",
        "        )\n",
        "\n",
        "        self.ffn_image = nn.Sequential(\n",
        "            nn.Linear(embed_dim, embed_dim * 4),\n",
        "            nn.GELU(),\n",
        "            nn.Dropout(dropout),\n",
        "            nn.Linear(embed_dim * 4, embed_dim),\n",
        "            nn.Dropout(dropout)\n",
        "        )\n",
        "\n",
        "        # Layer normalization\n",
        "        self.norm_text_1 = nn.LayerNorm(embed_dim)\n",
        "        self.norm_text_2 = nn.LayerNorm(embed_dim)\n",
        "        self.norm_image_1 = nn.LayerNorm(embed_dim)\n",
        "        self.norm_image_2 = nn.LayerNorm(embed_dim)\n",
        "\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, image_feats, text_feats):\n",
        "        # Text attends to image\n",
        "        text_attended, _ = self.text_to_image(\n",
        "            query=text_feats, key=image_feats, value=image_feats\n",
        "        )\n",
        "        text_feats = self.norm_text_1(text_feats + self.dropout(text_attended))\n",
        "        text_feats = self.norm_text_2(text_feats + self.ffn_text(text_feats))\n",
        "\n",
        "        # Image attends to text\n",
        "        image_attended, _ = self.image_to_text(\n",
        "            query=image_feats, key=text_feats, value=text_feats\n",
        "        )\n",
        "        image_feats = self.norm_image_1(image_feats + self.dropout(image_attended))\n",
        "        image_feats = self.norm_image_2(image_feats + self.ffn_image(image_feats))\n",
        "\n",
        "        # Pool and concatenate\n",
        "        image_pooled = image_feats.mean(dim=1)\n",
        "        text_pooled = text_feats.mean(dim=1)\n",
        "        fused_features = torch.cat([image_pooled, text_pooled], dim=1)\n",
        "\n",
        "        return fused_features, image_pooled, text_pooled\n",
        "\n",
        "\n",
        "# ===================== Enhanced Loss Function with Class Weights =====================\n",
        "class EnhancedLoss(nn.Module):\n",
        "    def __init__(self, contrastive_weight=0.04, temperature=0.07, class_weights=None):\n",
        "        super(EnhancedLoss, self).__init__()\n",
        "        # Use weighted cross entropy if class imbalance exists\n",
        "        if class_weights is not None:\n",
        "            self.ce_loss = nn.CrossEntropyLoss(weight=class_weights)\n",
        "        else:\n",
        "            self.ce_loss = nn.CrossEntropyLoss()\n",
        "        self.contrastive_weight = contrastive_weight  # Reduced from 0.1 to 0.05\n",
        "        self.temperature = temperature\n",
        "\n",
        "    def forward(self, logits, labels, image_feats, text_feats):\n",
        "        ce_loss = self.ce_loss(logits, labels)\n",
        "\n",
        "        # Contrastive alignment\n",
        "        image_feats_norm = F.normalize(image_feats, dim=-1)\n",
        "        text_feats_norm = F.normalize(text_feats, dim=-1)\n",
        "        similarity = torch.matmul(image_feats_norm, text_feats_norm.T) / self.temperature\n",
        "\n",
        "        batch_size = image_feats.size(0)\n",
        "        labels_contrastive = torch.arange(batch_size).to(image_feats.device)\n",
        "\n",
        "        contrastive_loss = (\n",
        "            F.cross_entropy(similarity, labels_contrastive) +\n",
        "            F.cross_entropy(similarity.T, labels_contrastive)\n",
        "        ) / 2\n",
        "\n",
        "        total_loss = ce_loss + self.contrastive_weight * contrastive_loss\n",
        "        return total_loss, ce_loss, contrastive_loss\n",
        "\n",
        "\n",
        "# ===================== Main Model with MuRIL =====================\n",
        "class CustomCLIPMuRILModel(nn.Module):\n",
        "    \"\"\"\n",
        "    CLIP + MuRIL model for Hinglish meme sentiment classification.\n",
        "    MuRIL is specifically designed for Indian languages and code-mixing.\n",
        "    \"\"\"\n",
        "    def __init__(self, clip_model, muril_model):\n",
        "        super(CustomCLIPMuRILModel, self).__init__()\n",
        "        self.clip_model = clip_model\n",
        "        self.muril_model = muril_model\n",
        "\n",
        "        # Make models trainable\n",
        "        for param in self.clip_model.parameters():\n",
        "            param.requires_grad = True\n",
        "        for param in self.muril_model.parameters():\n",
        "            param.requires_grad = True\n",
        "\n",
        "        # Project CLIP vision features to MuRIL dimension (768)\n",
        "        self.image_proj = nn.Sequential(\n",
        "            nn.Linear(768, 768),\n",
        "            nn.LayerNorm(768),\n",
        "            nn.GELU()\n",
        "        )\n",
        "\n",
        "        # Cross-attention fusion\n",
        "        self.cross_fusion = CrossAttentionFusion(embed_dim=768, num_heads=8, dropout=0.1)\n",
        "\n",
        "        # 4-layer MLP classifier with high dropout\n",
        "        self.classifier = nn.Sequential(\n",
        "            # Layer 1\n",
        "            nn.Linear(768 * 2, 768),\n",
        "            nn.LayerNorm(768),\n",
        "            nn.GELU(),\n",
        "            nn.Dropout(0.1),\n",
        "\n",
        "            # Layer 2\n",
        "            nn.Linear(768, 512),\n",
        "            nn.LayerNorm(512),\n",
        "            nn.GELU(),\n",
        "            nn.Dropout(0.2),\n",
        "\n",
        "            # Layer 3\n",
        "            nn.Linear(512, 256),\n",
        "            nn.LayerNorm(256),\n",
        "            nn.GELU(),\n",
        "            nn.Dropout(0.2),\n",
        "\n",
        "            # Layer 4\n",
        "            nn.Linear(256, 128),\n",
        "            nn.LayerNorm(128),\n",
        "            nn.GELU(),\n",
        "            nn.Dropout(0.1),\n",
        "\n",
        "            # Output layer\n",
        "            nn.Linear(128, 3)\n",
        "        )\n",
        "\n",
        "    def forward(self, image, input_ids, attention_mask):\n",
        "        # Extract CLIP vision features\n",
        "        vision_outputs = self.clip_model.vision_model(pixel_values=image)\n",
        "        image_features = vision_outputs.last_hidden_state  # [B, 50, 768]\n",
        "        image_features = self.image_proj(image_features)\n",
        "\n",
        "        # Extract MuRIL text features\n",
        "        muril_output = self.muril_model(\n",
        "            input_ids=input_ids,\n",
        "            attention_mask=attention_mask,\n",
        "            output_hidden_states=True\n",
        "        )\n",
        "        text_features = muril_output.last_hidden_state  # [B, seq_len, 768]\n",
        "\n",
        "        # Cross-attention fusion\n",
        "        fused_features, image_pooled, text_pooled = self.cross_fusion(\n",
        "            image_features, text_features\n",
        "        )\n",
        "\n",
        "        # Classification\n",
        "        logits = self.classifier(fused_features)\n",
        "        return logits, image_pooled, text_pooled\n",
        "\n",
        "\n",
        "# ===================== Utility Classes =====================\n",
        "class AvgMeter:\n",
        "    def __init__(self):\n",
        "        self.reset()\n",
        "\n",
        "    def reset(self):\n",
        "        self.val = 0\n",
        "        self.avg = 0\n",
        "        self.sum = 0\n",
        "        self.count = 0\n",
        "\n",
        "    def update(self, val, n=1):\n",
        "        self.val = val\n",
        "        self.sum += val * n\n",
        "        self.count += n\n",
        "        self.avg = self.sum / self.count\n",
        "\n",
        "\n",
        "def get_lr(optimizer):\n",
        "    for param_group in optimizer.param_groups:\n",
        "        return param_group['lr']\n",
        "\n",
        "\n",
        "# ===================== Training Function =====================\n",
        "def train_epoch(model, train_loader, optimizer, scheduler, device, criterion, scaler=None):\n",
        "    model.train()\n",
        "    loss_meter = AvgMeter()\n",
        "    ce_loss_meter = AvgMeter()\n",
        "    contrastive_loss_meter = AvgMeter()\n",
        "    correct_predictions = 0\n",
        "    total_predictions = 0\n",
        "\n",
        "    tqdm_object = tqdm(train_loader, total=len(train_loader))\n",
        "\n",
        "    for batch in tqdm_object:\n",
        "        images = batch['image'].to(device)\n",
        "        input_ids = batch['input_ids'].to(device)\n",
        "        attention_mask = batch['attention_mask'].to(device)\n",
        "        sentiments = batch['sentiment'].to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        if scaler is not None:\n",
        "            with autocast():\n",
        "                logits, image_feats, text_feats = model(images, input_ids, attention_mask)\n",
        "                loss, ce_loss, contrastive_loss = criterion(\n",
        "                    logits, sentiments, image_feats, text_feats\n",
        "                )\n",
        "\n",
        "            scaler.scale(loss).backward()\n",
        "            scaler.unscale_(optimizer)\n",
        "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
        "            scaler.step(optimizer)\n",
        "            scaler.update()\n",
        "        else:\n",
        "            logits, image_feats, text_feats = model(images, input_ids, attention_mask)\n",
        "            loss, ce_loss, contrastive_loss = criterion(\n",
        "                logits, sentiments, image_feats, text_feats\n",
        "            )\n",
        "            loss.backward()\n",
        "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
        "            optimizer.step()\n",
        "\n",
        "        scheduler.step()\n",
        "\n",
        "        count = images.size(0)\n",
        "        loss_meter.update(loss.item(), count)\n",
        "        ce_loss_meter.update(ce_loss.item(), count)\n",
        "        contrastive_loss_meter.update(contrastive_loss.item(), count)\n",
        "\n",
        "        preds = logits.argmax(dim=1)\n",
        "        correct_predictions += (preds == sentiments).sum().item()\n",
        "        total_predictions += sentiments.size(0)\n",
        "\n",
        "        tqdm_object.set_postfix(\n",
        "            train_loss=loss_meter.avg,\n",
        "            ce_loss=ce_loss_meter.avg,\n",
        "            contrast_loss=contrastive_loss_meter.avg,\n",
        "            lr=get_lr(optimizer)\n",
        "        )\n",
        "\n",
        "    accuracy = correct_predictions / total_predictions\n",
        "    return loss_meter, ce_loss_meter, contrastive_loss_meter, accuracy\n",
        "\n",
        "\n",
        "# ===================== Evaluation Function =====================\n",
        "def evaluate(model, data_loader, device, criterion):\n",
        "    model.eval()\n",
        "    predictions, true_labels = [], []\n",
        "    loss_meter = AvgMeter()\n",
        "    ce_loss_meter = AvgMeter()\n",
        "    contrastive_loss_meter = AvgMeter()\n",
        "    correct_predictions = 0\n",
        "    total_predictions = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch in data_loader:\n",
        "            images = batch['image'].to(device)\n",
        "            input_ids = batch['input_ids'].to(device)\n",
        "            attention_mask = batch['attention_mask'].to(device)\n",
        "            sentiments = batch['sentiment'].to(device)\n",
        "\n",
        "            logits, image_feats, text_feats = model(images, input_ids, attention_mask)\n",
        "            loss, ce_loss, contrastive_loss = criterion(\n",
        "                logits, sentiments, image_feats, text_feats\n",
        "            )\n",
        "\n",
        "            loss_meter.update(loss.item(), len(images))\n",
        "            ce_loss_meter.update(ce_loss.item(), len(images))\n",
        "            contrastive_loss_meter.update(contrastive_loss.item(), len(images))\n",
        "\n",
        "            preds = logits.argmax(dim=1)\n",
        "            correct_predictions += (preds == sentiments).sum().item()\n",
        "            total_predictions += sentiments.size(0)\n",
        "\n",
        "            predictions.extend(preds.cpu().numpy())\n",
        "            true_labels.extend(sentiments.cpu().numpy())\n",
        "\n",
        "    accuracy = correct_predictions / total_predictions\n",
        "    return predictions, true_labels, loss_meter.avg, ce_loss_meter.avg, contrastive_loss_meter.avg, accuracy\n",
        "\n",
        "\n",
        "# ===================== Visualization Functions =====================\n",
        "def plot_training_history(train_losses, val_losses, train_accuracies, val_accuracies):\n",
        "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))\n",
        "\n",
        "    ax1.plot(train_losses, label='Train Loss', marker='o')\n",
        "    ax1.plot(val_losses, label='Val Loss', marker='s')\n",
        "    ax1.set_xlabel('Epoch')\n",
        "    ax1.set_ylabel('Loss')\n",
        "    ax1.set_title('Training and Validation Loss')\n",
        "    ax1.legend()\n",
        "    ax1.grid(True)\n",
        "\n",
        "    ax2.plot(train_accuracies, label='Train Accuracy', marker='o')\n",
        "    ax2.plot(val_accuracies, label='Val Accuracy', marker='s')\n",
        "    ax2.set_xlabel('Epoch')\n",
        "    ax2.set_ylabel('Accuracy')\n",
        "    ax2.set_title('Training and Validation Accuracy')\n",
        "    ax2.legend()\n",
        "    ax2.grid(True)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.savefig('training_history_xlm.png', dpi=300, bbox_inches='tight')\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "def plot_confusion_matrix(true_labels, predictions, class_names=['Negative', 'Neutral', 'Positive']):\n",
        "    cm = confusion_matrix(true_labels, predictions)\n",
        "    plt.figure(figsize=(8, 6))\n",
        "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
        "                xticklabels=class_names, yticklabels=class_names)\n",
        "    plt.title('Confusion Matrix - XLM-RoBERTa')\n",
        "    plt.ylabel('True Label')\n",
        "    plt.xlabel('Predicted Label')\n",
        "    plt.savefig('confusion_matrix_xlm.png', dpi=300, bbox_inches='tight')\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "# ===================== Main Training Script =====================\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "# Data augmentation\n",
        "train_image_transforms = A.Compose([\n",
        "    A.Resize(224, 224),\n",
        "    A.HorizontalFlip(p=0.5),\n",
        "    A.RandomBrightnessContrast(brightness_limit=0.2, contrast_limit=0.2, p=0.3),\n",
        "    A.ShiftScaleRotate(shift_limit=0.1, scale_limit=0.1, rotate_limit=15, p=0.5),\n",
        "    A.CoarseDropout(max_holes=8, max_height=16, max_width=16, p=0.2),\n",
        "    A.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
        "])\n",
        "\n",
        "val_image_transforms = A.Compose([\n",
        "    A.Resize(224, 224),\n",
        "    A.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
        "])\n",
        "\n",
        "\n",
        "# Load models\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"LOADING MODELS: CLIP + MuRIL\")\n",
        "print(\"=\"*70)\n",
        "print(\"Loading CLIP (Vision Encoder)...\")\n",
        "clip_model = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
        "\n",
        "print(\"Loading MuRIL (Hinglish Text Encoder)...\")\n",
        "muril_tokenizer = AutoTokenizer.from_pretrained('google/muril-base-cased')\n",
        "muril_model = AutoModel.from_pretrained('google/muril-base-cased')\n",
        "\n",
        "print(\"âœ“ Models loaded successfully!\")\n",
        "print(f\"  CLIP: {sum(p.numel() for p in clip_model.parameters()):,} parameters\")\n",
        "print(f\"  MuRIL (uncased): {sum(p.numel() for p in muril_model.parameters()):,} parameters\")\n",
        "print(\"\\nðŸ’¡ MuRIL-uncased Features:\")\n",
        "print(\"  - Trained on 17 Indian languages\")\n",
        "print(\"  - Excellent Hinglish support\")\n",
        "print(\"  - Handles case-insensitive text (perfect for memes)\")\n",
        "print(\"  - Understands code-mixing naturally\")\n",
        "print(\"  - Indian cultural context awareness\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "# Create model\n",
        "model = CustomCLIPMuRILModel(clip_model, muril_model)\n",
        "model = model.to(device)\n",
        "\n",
        "total_params = sum(p.numel() for p in model.parameters())\n",
        "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "\n",
        "print(f\"\\nModel Statistics:\")\n",
        "print(f\"  Total parameters:     {total_params:,}\")\n",
        "print(f\"  Trainable parameters: {trainable_params:,}\")\n",
        "\n",
        "# Dataset paths\n",
        "train_image_dir = '/content/drive/MyDrive/memo_3/trainImages/trainImages'\n",
        "val_image_dir = '/content/drive/MyDrive/memo_3/valImages/valImages'\n",
        "\n",
        "# Create datasets\n",
        "print(\"\\nCreating datasets with MuRIL tokenizer...\")\n",
        "train_dataset = MemeDataset(\n",
        "    images=train_df['image_url'].tolist(),\n",
        "    captions=train_df['ocr'].tolist(),\n",
        "    sentiments=train_df['overall'].tolist(),\n",
        "    tokenizer=muril_tokenizer,  # Using MuRIL tokenizer\n",
        "    image_transforms=train_image_transforms,\n",
        "    image_dir=train_image_dir\n",
        ")\n",
        "\n",
        "val_dataset = MemeDataset(\n",
        "    images=val_df['image_url'].tolist(),\n",
        "    captions=val_df['ocr'].tolist(),\n",
        "    sentiments=val_df['overall'].tolist(),\n",
        "    tokenizer=muril_tokenizer,  # Using MuRIL tokenizer\n",
        "    image_transforms=val_image_transforms,\n",
        "    image_dir=val_image_dir\n",
        ")\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True, num_workers=4)\n",
        "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False, num_workers=4)\n",
        "\n",
        "# CRITICAL: Check class distribution and compute class weights\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"CLASS DISTRIBUTION ANALYSIS\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "train_class_counts = train_df['overall'].value_counts().sort_index()\n",
        "val_class_counts = val_df['overall'].value_counts().sort_index()\n",
        "\n",
        "print(\"\\nTraining Set:\")\n",
        "for class_idx in range(3):\n",
        "    count = train_class_counts.get(class_idx, 0)\n",
        "    percentage = (count / len(train_df)) * 100\n",
        "    print(f\"  Class {class_idx}: {count:,} samples ({percentage:.2f}%)\")\n",
        "\n",
        "print(\"\\nValidation Set:\")\n",
        "for class_idx in range(3):\n",
        "    count = val_class_counts.get(class_idx, 0)\n",
        "    percentage = (count / len(val_df)) * 100\n",
        "    print(f\"  Class {class_idx}: {count:,} samples ({percentage:.2f}%)\")\n",
        "\n",
        "# Compute class weights\n",
        "class_counts = torch.tensor([train_class_counts.get(i, 1) for i in range(3)], dtype=torch.float32)\n",
        "class_weights = 1.0 / class_counts\n",
        "class_weights = class_weights / class_weights.sum() * 3\n",
        "class_weights = class_weights.to(device)\n",
        "\n",
        "print(f\"\\nComputed Class Weights: {class_weights.cpu().numpy()}\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "# Optimizer with HIGHER learning rates for better convergence\n",
        "optimizer = torch.optim.AdamW([\n",
        "    {'params': model.clip_model.parameters(), 'lr': 2e-6},\n",
        "    {'params': model.muril_model.parameters(), 'lr': 2e-6},\n",
        "    {'params': model.image_proj.parameters(), 'lr': 5e-4},\n",
        "    {'params': model.cross_fusion.parameters(), 'lr': 5e-4},\n",
        "    {'params': model.classifier.parameters(), 'lr': 5e-4}\n",
        "], weight_decay=1e-4)\n",
        "\n",
        "num_epochs = 10\n",
        "scheduler = get_linear_schedule_with_warmup(\n",
        "    optimizer,\n",
        "    num_warmup_steps=len(train_loader) * 3,\n",
        "    num_training_steps=len(train_loader) * num_epochs\n",
        ")\n",
        "\n",
        "# Simple classification loss\n",
        "criterion = EnhancedLoss(\n",
        "    contrastive_weight=0.04,  # Reduced from 0.1\n",
        "    temperature=0.07,\n",
        "    class_weights=class_weights  # Add class weights\n",
        ")\n",
        "scaler = GradScaler() if device == 'cuda' else None\n",
        "\n",
        "print(f\"\\n{'='*70}\")\n",
        "print(\"LOSS CONFIGURATION\")\n",
        "print(f\"{'='*70}\")\n",
        "print(\"Using: Pure Cross-Entropy Loss (NO contrastive loss)\")\n",
        "print(\"Focus: Sentiment classification only\")\n",
        "print(f\"{'='*70}\\n\")\n",
        "\n",
        "# Training tracking\n",
        "BEST_MODEL_PATH = 'best_model_muril.pth'\n",
        "train_losses = []\n",
        "val_losses = []\n",
        "train_accuracies = []\n",
        "val_accuracies = []\n",
        "best_val_accuracy = 0\n",
        "\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"STARTING TRAINING WITH CLIP + MuRIL\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    print(f\"\\n{'='*50}\")\n",
        "    print(f\"Epoch {epoch + 1}/{num_epochs}\")\n",
        "    print(f\"{'='*50}\")\n",
        "\n",
        "    # Train - FIXED: Unpack all 4 return values\n",
        "    train_loss_meter, train_ce_loss, train_contrastive_loss, train_accuracy = train_epoch(\n",
        "        model, train_loader, optimizer, scheduler, device, criterion, scaler\n",
        "    )\n",
        "\n",
        "    # Validate - FIXED: Unpack all 6 return values\n",
        "    val_predictions, val_true_labels, val_loss, val_ce_loss, val_contrastive_loss, val_accuracy = evaluate(\n",
        "        model, val_loader, device, criterion\n",
        "    )\n",
        "\n",
        "    # Save best model\n",
        "    if val_accuracy > best_val_accuracy:\n",
        "        best_val_accuracy = val_accuracy\n",
        "        torch.save({\n",
        "            'epoch': epoch,\n",
        "            'model_state_dict': model.state_dict(),\n",
        "            'optimizer_state_dict': optimizer.state_dict(),\n",
        "            'val_accuracy': val_accuracy,\n",
        "        }, BEST_MODEL_PATH)\n",
        "        print(f\"âœ“ Best model saved with validation accuracy: {val_accuracy:.4f}\")\n",
        "\n",
        "    # Store metrics\n",
        "    train_losses.append(train_loss_meter.avg)\n",
        "    val_losses.append(val_loss)\n",
        "    train_accuracies.append(train_accuracy)\n",
        "    val_accuracies.append(val_accuracy)\n",
        "\n",
        "    # Print summary\n",
        "    print(f\"\\nEpoch {epoch + 1} Summary:\")\n",
        "    print(f\"  Train Loss: {train_loss_meter.avg:.4f}\")\n",
        "    print(f\"  Train CE Loss: {train_ce_loss.avg:.4f}\")\n",
        "    print(f\"  Train Contrastive Loss: {train_contrastive_loss.avg:.4f}\")\n",
        "    print(f\"  Train Accuracy: {train_accuracy:.4f}\")\n",
        "    print(f\"  Val Loss: {val_loss:.4f}\")\n",
        "    print(f\"  Val CE Loss: {val_ce_loss:.4f}\")\n",
        "    print(f\"  Val Contrastive Loss: {val_contrastive_loss:.4f}\")\n",
        "    print(f\"  Val Accuracy: {val_accuracy:.4f}\")\n",
        "    print(f\"  Best Val Accuracy: {best_val_accuracy:.4f}\")\n",
        "\n",
        "# Load best model\n",
        "print(f\"\\nLoading best model with validation accuracy: {best_val_accuracy:.4f}\")\n",
        "checkpoint = torch.load(BEST_MODEL_PATH)\n",
        "model.load_state_dict(checkpoint['model_state_dict'])\n"
      ],
      "metadata": {
        "id": "dHvH6FfguXsp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Final evaluation on validation set\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"FINAL EVALUATION ON VALIDATION SET\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "val_predictions, val_true_labels, val_loss, val_ce_loss, val_contrastive_loss, val_accuracy = evaluate(\n",
        "    model, val_loader, device, criterion\n",
        ")\n",
        "\n",
        "print(f\"\\nFinal Validation Accuracy: {val_accuracy:.4f}\")\n",
        "print(f\"Final Validation Loss: {val_loss:.4f}\")\n",
        "\n",
        "# Classification report\n",
        "print(\"\\nClassification Report:\")\n",
        "print(classification_report(\n",
        "    val_true_labels,\n",
        "    val_predictions,\n",
        "    target_names=['Negative', 'Neutral', 'Positive']\n",
        "))\n",
        "\n",
        "# F1 scores\n",
        "f1_macro = f1_score(val_true_labels, val_predictions, average='macro')\n",
        "f1_weighted = f1_score(val_true_labels, val_predictions, average='weighted')\n",
        "print(f\"F1 Score (Macro): {f1_macro:.4f}\")\n",
        "print(f\"F1 Score (Weighted): {f1_weighted:.4f}\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"TRAINING COMPLETE!\")\n",
        "print(\"=\"*70)"
      ],
      "metadata": {
        "id": "_Oa0cNZbuXwM"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}